{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRI Image Classification With CNN (CS584, Please do not re-post on the web!)\n",
    "\n",
    "In this tutorial, we show you step-by-step how to classify a set of 3D MRI images into two main classes: Non-stroke and Stroke. We describe how you can read the 3D-MRI data and how you can use your own data for learning of a deep architecture like a Convolutional Neural Network (CNN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset for this project are 202 synthetic image sequences, generated to closely resemble real-world examples of Non-stroke and Stroked brain on Apparent Diffusion Coefficient (ADC) maps; a type of post-processed MRI image sequence (See https://www.youtube.com/watch?v=MMyeQpHLeng&t=6s). Each image sequence is classified by an expert into either Non-stroke or Stroke class (0 v.s. 1, respectively). Starting from the original 202 image sequences, we applied 'Data Augmentation' to increase the dataset size to 808 image sequences of size 128x128x24 (image resolution: 128x128, slice/sequence resolution: 24). Here we only performed basic image rotation (-+5 degrees), flipping, and rotation+flipping. But, you may apply other types of data augmentation to increase the size of your dataset.\n",
    "\n",
    "* For more information on MRI images see:\n",
    "https://www.youtube.com/watch?v=mOt2FeGHjaY\n",
    "https://bmcmedimaging.biomedcentral.com/articles/10.1186/1471-2342-11-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Libraries\n",
    "\n",
    "Loads all the required libraries and initialize certain parameters related to the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from sklearn import metrics\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# image dimensions\n",
    "n_depth = 24\n",
    "n_input_x = 128\n",
    "n_input_y = 128\n",
    "n_classes = 2\n",
    "\n",
    "Counter_batch = 0\n",
    "#Maximum iteration for training\n",
    "max_iter = 10\n",
    "#Batch size\n",
    "batch_sz = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Path to Images\n",
    "\n",
    "There are two different CSV files one containing the name of each image and the other one contains the corresponding label. In this stage, we find the path of each image and saved the results to \"pathDicom3\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_Files = []\n",
    "All_labels = []\n",
    "\n",
    "with open('/home/class/cs584/stroke_data_augmented_outcomes.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        All_Files.append(row[0])\n",
    "        All_labels.append(row[2])\n",
    "\n",
    "\n",
    "All_Files_202 = []\n",
    "All_labels_202 = []\n",
    "\n",
    "with open('/home/class/cs584/stroke_data_outcomes.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        All_Files_202.append(row[0])\n",
    "        All_labels_202.append(row[2])\n",
    "\n",
    "File_exclude_Ind = [7,12,37,40,51,76,79,80,88,94,96,101,102,110,159,164,181,200]\n",
    "\n",
    "\n",
    "File_exclude = []\n",
    "\n",
    "for i in range(len(File_exclude_Ind)):\n",
    "    File_exclude.append(All_Files_202[File_exclude_Ind[i]])\n",
    "\n",
    "\n",
    "print 'Number of File Exclude: %g' % len(File_exclude)\n",
    "\n",
    "pathDicom = os.listdir(\"/home/class/cs584/synthetic_images/\")\n",
    "pathDicom3 = []\n",
    "item_values = []\n",
    "\n",
    "\n",
    "for item in pathDicom:\n",
    "    check = 0\n",
    "    for i in range(len(File_exclude)):\n",
    "        if File_exclude[i] in item:\n",
    "            check = 1\n",
    "            break\n",
    "        else:\n",
    "            check = 0\n",
    "\n",
    "    if check == 0:\n",
    "        item2 = '/home/class/cs584/synthetic_images/' + item\n",
    "        item_values.append(item)\n",
    "        pathDicom2 = os.listdir(item2)\n",
    "        substring1 = 'ADC'\n",
    "        substring2 = 'Apparent_Diffusion_Coefficient_(mm2s)'\n",
    "        gg = 0\n",
    "        for i in range(len(pathDicom2)):\n",
    "            if substring1 in pathDicom2[i]:\n",
    "                item3 = item2 + '/' + pathDicom2[i]\n",
    "                gg += 1\n",
    "                break\n",
    "\n",
    "            if substring2 in pathDicom2[i]:\n",
    "                item3 = item2 + '/' + pathDicom2[i]\n",
    "                gg += 1\n",
    "                break\n",
    "\n",
    "        pathDicom3.append(item3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading of the Dataset\n",
    "\n",
    "In this step, 3D-MRI images are being read from the text files and the results will be saved in a 3D-matrix. It must be considered that each image is a 3D matrix of size (128x128x24). Finally, this data will be used for learning of Convolutional Neural Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Input_data = []\n",
    "label = []\n",
    "count = -1\n",
    "for item in pathDicom3:\n",
    "    count = count + 1\n",
    "    try:\n",
    "        item_index = All_Files.index(item_values[count])\n",
    "        label.append(All_labels[item_index])\n",
    "    except:\n",
    "        f = 0\n",
    "\n",
    "n_examples = 100\n",
    "\n",
    "# Assign the Actual label value\n",
    "Actual_Label = np.zeros(n_examples, dtype= int)\n",
    "for i in range(n_examples):\n",
    "    if label[i] == '1':\n",
    "        Actual_Label[i] = 1\n",
    "    else:\n",
    "        Actual_Label[i] = 0\n",
    "\n",
    "FeedData = []\n",
    "Files = []\n",
    "P_Files = []\n",
    "for num in range (100):\n",
    "        for i in range (n_depth):\n",
    "\n",
    "                f = open(pathDicom3[num] + \"/IM-\"+str(i+1)+\".txt\")\n",
    "                for row in f.readlines():\n",
    "                    for word in row.split(','):\n",
    "                       if word != '0\\n':\n",
    "                           if word == '0':\n",
    "                                Files.append(int(word))\n",
    "                           else:\n",
    "                               Files.append(float(word))\n",
    "                       else:\n",
    "\n",
    "                           Files.append(0)\n",
    "\n",
    "                Files_Array = np.array(Files)\n",
    "                Files_Array = np.float32(Files_Array)\n",
    "\n",
    "                Files = []\n",
    "                Files_Image = np.reshape(Files_Array, (n_input_x, n_input_y))\n",
    "                P_Files.append(Files_Image)\n",
    "\n",
    "        FeedData.append(P_Files)\n",
    "        P_Files = []\n",
    "\n",
    "FeedData1 = np.array(FeedData)\n",
    "\n",
    "\n",
    "del FeedData\n",
    "#normalization\n",
    "size = FeedData1.shape\n",
    "\n",
    "print size\n",
    "\n",
    "\n",
    "MAX_Dataset = np.amax(FeedData1)\n",
    "Normalized_FeedData1 = FeedData1/ MAX_Dataset\n",
    "\n",
    "Data_Input = FeedData1.reshape((n_examples, n_depth, n_input_x, n_input_y, 1))\n",
    "# TF Section\n",
    "del FeedData1\n",
    "\n",
    "label_data = np.zeros([n_examples, 2])\n",
    "\n",
    "for i in range(n_examples):\n",
    "    if label[i] == '1':\n",
    "        label_data[i, 1] = 1\n",
    "    else:\n",
    "        label_data[i, 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Convolutional Layers\n",
    "\n",
    "In this step, we need to define the various parameters of each layer, such as the number of feature maps (or filters/kernels), parameters of each filter, biases, activation functions and pooling operations (max, average). Note that the filter parameters or weights are the convolutional filters that will be learned during the training process. We then put all of these components together to make a CNN layer. Finally, this procedure has to be repeated to make a CNN with an arbitrary number of layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, seed = 10000)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Number of filters at the first Conv layer\n",
    "N_Conv_layer1 = 5\n",
    "\n",
    "# Output size of the Fully Connected layer\n",
    "N_W_fc1 = 50\n",
    "N_b_fc1 = 50\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_depth, n_input_x, n_input_y, 1])\n",
    "y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "W_cv1 = weight_variable([3, 3, 3, 1, N_Conv_layer1])\n",
    "b_cv1 = bias_variable([N_Conv_layer1])\n",
    "\n",
    "conv1 = tf.nn.conv3d(x, W_cv1, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
    "conv1 = tf.nn.bias_add(conv1, b_cv1)\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "conv1 = tf.nn.max_pool3d(conv1, ksize=[1, 4, 4, 4, 1], strides=[1, 4, 4, 4, 1], padding=\"SAME\")\n",
    "\n",
    "# MLP\n",
    "W_fc1 = weight_variable([6 * 32 * 32 * N_Conv_layer1, N_W_fc1])\n",
    "b_fc1 = bias_variable([N_b_fc1])\n",
    "\n",
    "h_pool2_flat = tf.reshape(conv1, [-1, 6 * 32 * 32 * N_Conv_layer1])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "W_fc2 = weight_variable([N_W_fc1, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "y_conv = (tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "#Regularization\n",
    "reg = (tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(b_fc1) + tf.nn.l2_loss(W_fc2) + tf.nn.l2_loss(b_fc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer\n",
    "\n",
    "In this section, we define the loss function and an optimizer for training section. According to the code bellow, the \"softmax_cross_entropy\" uses as loss function and \"AdamOptimizer\" for optimization algorithm. The accuracy can be computed based on the third and fourth lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_)) + 0.1 * reg\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "Y_softmax = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation and Test Set\n",
    "\n",
    "Before feeding the data into the network, we have to separate entire dataset into these three main sections. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Training = []\n",
    "Testing = []\n",
    "training_l = []\n",
    "test_l = []\n",
    "\n",
    "Training_set = Data_Input[:60,]\n",
    "training_labels = label_data[:60]\n",
    "Valid_set = Data_Input[60:80,]\n",
    "Valid_label = label_data[60:80]\n",
    "Test_set = Data_Input[80:,]\n",
    "Test_label = label_data[80:]\n",
    "\n",
    "del Data_Input\n",
    "del label_data\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "n_batches = int(len(Training_set)/batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the Network\n",
    "\n",
    "In this section, the data is broken into different mini-batches, then each batch feeds into the network separately. The defined Optimizer optimizes the weight values in each iteration. In this step, the weights will be updated by optimizer according to the defined the loss function. This procedure is called backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_accuracy = np.zeros(len(range(max_iter))).astype(float)\n",
    "valid_accuracy = np.zeros(len(range(max_iter))).astype(float)\n",
    "Test_Accuracy = np.zeros(len(range(max_iter))).astype(float)\n",
    "val_smooth = np.zeros(len(range(max_iter))).astype(float)\n",
    "tempt = np.zeros(len(range(max_iter))).astype(float)\n",
    "AUC0 = np.zeros(len(range(max_iter))).astype(float)\n",
    "AUC1 = np.zeros(len(range(max_iter))).astype(float)\n",
    "\n",
    "NUM_THREADS = 120\n",
    "with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=NUM_THREADS,inter_op_parallelism_threads=NUM_THREADS,log_device_placement=False)) as session:\n",
    "    session.run(init)\n",
    "\n",
    "    for i in xrange(max_iter):\n",
    "        Xtrain_temp, Ytrain_temp = shuffle(Training_set, training_labels)\n",
    "        shuf2 = shuffle(range(n_batches))\n",
    "        n_batches_half = int(n_batches/n_batches)\n",
    "        for j in xrange(n_batches_half):\n",
    "\n",
    "            batch1 = Xtrain_temp[shuf2[j] * batch_sz:(shuf2[j] * batch_sz + batch_sz), ]\n",
    "            batch2 = Ytrain_temp[shuf2[j] * batch_sz:(shuf2[j] * batch_sz + batch_sz), ]\n",
    "\n",
    "\n",
    "            if j%5 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x: batch1, y_: batch2})\n",
    "                print(\"Step %d, BatchNumber: %d, training accuracy %g\" % (i, j, train_accuracy))\n",
    "\n",
    "            optimizer.run(feed_dict={x: batch1, y_: batch2})\n",
    "\n",
    "        n_batches_train = int(len(training_labels) / batch_sz)\n",
    "        n_batches_test = int(len(Test_label) / batch_sz)\n",
    "        n_batches_val = int(len(Valid_label) / batch_sz)\n",
    "\n",
    "        training_acc = np.zeros(len(range(n_batches_train))).astype(float)\n",
    "        valid_acc = np.zeros(len(range(n_batches_val))).astype(float)\n",
    "        Test_Acc = np.zeros(len(range(n_batches_test))).astype(float)\n",
    "\n",
    "        for j in xrange(n_batches_train):\n",
    "            batch_train1 = Training_set[j * batch_sz:(j * batch_sz + batch_sz), ]\n",
    "            batch_train2 = training_labels[j * batch_sz:(j * batch_sz + batch_sz), ]\n",
    "            training_acc[j] = accuracy.eval(feed_dict={x: batch_train1, y_: batch_train2})\n",
    "\n",
    "        training_accuracy[i] = np.mean(training_acc)\n",
    "\n",
    "        for j in xrange(n_batches_test):\n",
    "            batch_test1 = Test_set[j * batch_sz:(j * batch_sz + batch_sz), ]\n",
    "            batch_test2 = Test_label[j * batch_sz:(j * batch_sz + batch_sz), ]\n",
    "            start_time = time.time()\n",
    "            Test_Acc[j] = accuracy.eval(feed_dict={x: batch_test1, y_: batch_test2})\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        Test_Accuracy[i] = np.mean(Test_Acc)\n",
    "\n",
    "        for j in xrange(n_batches_val):\n",
    "            batch_val1 = Valid_set[j * batch_sz:(j * batch_sz + batch_sz), ]\n",
    "            batch_val2 = Valid_label[j * batch_sz:(j * batch_sz + batch_sz), ]\n",
    "            valid_acc[j] = accuracy.eval(feed_dict={x: batch_val1, y_: batch_val2})\n",
    "\n",
    "        valid_accuracy[i] = np.mean(valid_acc)\n",
    "\n",
    "\n",
    "        if i == 0:\n",
    "            val_smooth[i] = valid_accuracy[i]\n",
    "\n",
    "        if (i >= 1):\n",
    "            val_smooth[i] = 0.7 * valid_accuracy[i - 1] + 0.3 * valid_accuracy[i]\n",
    "\n",
    "        # lable contains the actual class labels\n",
    "        Actual_Label_Test = Actual_Label[80:]\n",
    "        AUC = y_conv.eval(feed_dict={x: Test_set, y_: Test_label})\n",
    "        fpr0, tpr0, thresholds0 = metrics.roc_curve(Actual_Label_Test, AUC[:, 0], pos_label=0)\n",
    "        fpr1, tpr1, thresholds1 = metrics.roc_curve(Actual_Label_Test, AUC[:, 1], pos_label=1)\n",
    "        AUC0[i] = metrics.auc(fpr0, tpr0)\n",
    "        AUC1[i] = metrics.auc(fpr1, tpr1)\n",
    "        print('============================================================')\n",
    "        print('Training Accuracy: %g' % training_accuracy[i])\n",
    "        print('Validation Accuracy: %g' % val_smooth[i])\n",
    "        print ('Test Accuracy: %g' % Test_Accuracy[i])\n",
    "        print('AUC Measure Class 0: %g' % AUC0[i])\n",
    "        print('AUC Measure Class 1: %g' % AUC1[i])\n",
    "        print(\"============================================================\")\n",
    "        save_path = saver.save(session, str(i) + \"Checkpoint.ckpt\")\n",
    "        print \"Fast2\"\n",
    "\n",
    "\n",
    "    np.savetxt(\"Validation-Fast2.csv\", val_smooth, delimiter=\",\")\n",
    "    np.savetxt(\"train-Fast2.csv\", training_accuracy, delimiter=\",\")\n",
    "    np.savetxt(\"Test-Fast2.csv\", Test_Accuracy, delimiter=\",\")\n",
    "    np.savetxt(\"AUC0.csv\", AUC0, delimiter=\",\")\n",
    "    np.savetxt(\"AUC1.csv\", AUC1, delimiter=\",\")    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
